[["index.html", "Recoding Bayesian Cognitive Modeling in Stan Welcome Why this? My assumptions about you How to use and understand this project Times change and so do we Acknowledgments License and citation", " Recoding Bayesian Cognitive Modeling in Stan Riccardo Fusaroli 2021-11-08 Welcome Michael Lee and EJ Wagenmakers’ Bayesian Cognitive Modeling text, (https://bayesmodels.com/), is a common entry to computational cognitive modeling, also thanks to the summer school Lee and Wagenmakers regularly hold in Amsterdam. The book presents code in WinBUG and JAGS. This project is an effort to contextualize the models presented in the book in a more up-to-date Bayesian Workflow relying on Stan. But to be clear, this project is not meant to stand alone. It’s a supplement to the textbook. The source code of the project is available here. This project is powered by the great Yihui Xie’s bookdown package [@R-bookdown; @xieBookdownAuthoringBooks2016], which makes it easy to turn R markdown files into HTML, PDF, and EPUB. You might also check out Xie, Allaire, and Grolemund’s [-@xieMarkdownDefinitiveGuide2020] R markdown: The definitive guide. Why this? i want to get better at cognitive modeling and Stan is my drug of choice. I will therefore recode each model directly in Stan, calling the model in R via cmdstanr. As a comparison, I’ll also recode most models in BRMS to compare the automatically generated Stan code - and showcase how awesome BRMS is. My assumptions about you TBA How to use and understand this project This project is not meant to stand alone. It’s a supplement to the original book. I roughl follow the structure of the text, translating the models into Stan, cmdstanr and brms code. However, I am not bound to the exact sequence of steps in each chapter. Times change and so do we For a brief rundown of the version history, we have: 0.1.0. The current incomplete version. What’s next? I hope to provide a full annotated recoding of the book by the end of Fall ’21. We’ll see. In the meantime, feel free to make suggestions or report typos and coding goofs at https://github.com/fusaroli/RecodingBayesianCognitiveModeling/issues. Acknowledgments This project is largely inspired by Solomon Kurz’s generous recoding of statistical books in brms and tidyverse: https://solomonkurz.netlify.app/bookdown/. My Stan code often relies on an earlier Stan recoding of the book by Martin Smira. The code is a bit outdated and keeps closer to the original winBUG code, but I’ve abundantly relied on it to get unstuck. License and citation This book is licensed under the Creative Commons Zero v1.0 Universal license. In short, you can use my work. Just make sure you give me the appropriate credit the same way you would for any other scholarly resource. Here’s the citation information: TBA "],["inferring-rates.html", "Chapter 1 Inferring rates", " Chapter 1 Inferring rates Lee and Wagenmakers introduce us to modeling with a series of examples of inferring rates. Given a certain amount of trials n, we observe a certain amount of successes k. E.g. imagine testing your professor with a series of yees/no questions to assess their ability in cognitive modeling. The amount of questions is n, the amount of correct answers is k. Yet, we are not really interested in k. We are interested in the underlying ability of the professor to answer this kind of questions, as a proxy for their knowledge/skills in this domain. This underlying unobserved variable is the rate or theta. Theta is of interest because it goes beyond the observed data and allows us to predict, e.g. how many answers would be correct in a new assessment of the professor with a different amount of questions. Analogously, theta allows us to compare this professor’s skills with another one’s skills even if they answer a different amount of questions. The exercises present 3 modeling issues: 1. Inferring the posterior distribution of a rate θ after having observed k successes from n trials. 2. Inferring the posterior distribution of the difference between two rates (θ1 and θ2) after having observed two different processes, producing k1 and k2 successes out of n1 and n2 trials, respectively. 3. Inferring the posterior distribution of a rate θ after having observed two instances of the rate in action: two binary processes, producing k1 and k2 successes out of n1 and n2 trials, respectively. For each of these exercises, I’ll first generate the data, then setup the model, then add the code necessary to do proper quality checks of the model: prior predictive checks, posterior predictive checks, prior/posterior update checks. First we want to generate the data. Note that usually we would want to set a fixed theta, then generate the predicted k successes given n trials, since that’d allow to see how well the model recovers the underlying theta. But for now we follow the book. # underlying rate theta &lt;- 0.6 # amount of trials n &lt;- 10 # generating successes (nb the book would say 7) k &lt;- rbinom(1, size = n, prob = theta) ## Create the data data &lt;- list( n = n, k = k ) Second we want to define a basic rate inferring model. A stan program is composed (in its minimal form) by 3 blocks: data, parameters and model. data identifies the variables that are passed to the model and defines their type (e.g. whether they are real or integer numbers) and whether they are bounded (e.g. cannot be below 0). Here it’s easy, we have 2 variables that we are observing: the number of trials (n) and the number of success (k). Both are integers, we need to observe at least 1 trial, or there’s no data (n&gt;0) and there cannot be a negative number of successes (k&gt;=0). parameters identifies the parameters that need to be inferred thorugh the model. In this case, we only have one parameter: rate, which is bound between 0 (never successes) and 1 (always successes). model includes two parts: the priors for each of the parameters and the likelihood formula(s). Here, again, we have the prior for our one parameter theta and the likelihood stating that k is generated according to a binomial distribution with a number of trial n and a rate theta. library(cmdstanr) stan_file &lt;- write_stan_file(&quot; // This Stan model infers a rate (theta) from a number of trials (n) and successes (k) // The input data is two integer numbers: n and k. data { int&lt;lower=1&gt; n; // n of trials (there has to be at least 1 to have observable data) int&lt;lower=0&gt; k; // n of successes (lowest n is no successes) } // The parameters accepted by the model. Our model accepts only theta, the rate, // which is bound at 0 (no chances of success) and 1 (always success) parameters { real&lt;lower=0, upper=1&gt; theta; } // The model to be estimated; prior and likelihood model { // The prior for theta is a uniform distribution between 0 and 1 theta ~ beta(1, 1); // The model consists in a binomial distribution with a rate theta, // and a number of trials n generating k successes k ~ binomial(n, theta); } &quot;) Then we fit the model and assess the estimates mod &lt;- cmdstan_model(stan_file, cpp_options = list(stan_threads = TRUE), pedantic = TRUE) ## - \\ | / - \\ | / - \\ | / - \\ | / - \\ | / - \\ | / - \\ | / - \\ | / - \\ | / - \\ | / - \\ | / - \\ | / - \\ | / - \\ | / - \\ | / - \\ | / samples &lt;- mod$sample( data = data, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99, ) ## Running MCMC with 2 parallel chains, with 2 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 1 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 1 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 1 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 1 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 1 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 1 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 1 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 1 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 1 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 2 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 2 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 2 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 2 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 2 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 2 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 2 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 2 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 2 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 2 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 1 finished in 0.0 seconds. ## Chain 2 finished in 0.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.0 seconds. ## Total execution time: 0.9 seconds. samples ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## lp__ -8.19 -7.90 0.72 0.36 -9.63 -7.64 1.00 1019 1047 ## theta 0.67 0.68 0.13 0.14 0.43 0.87 1.00 915 1018 As you know from the Bayesian workflow discussion [MISSING], there is a lot of model checking that we should do. We haven’t assessed our priors, nor the model posteriors against priors and against data, nor the quality of the model fitting process. "],["a-caucus-race-and-a-long-tale.html", "Chapter 2 A caucus-race and a long tale", " Chapter 2 A caucus-race and a long tale "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
